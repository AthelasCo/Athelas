{
  "name": "Athelas",
  "tagline": "",
  "body": "#A parallel random graph generation library.\r\n                                  By\r\n                   Karandeep Johar & Eshan Verma  \r\n             kjohar@andrew.cmu.edu   everma@andrew.cmu.edu\r\n\r\n###SUMMARY:\r\nWe will implement a library to generate parallel large-scale graphs using OpenMP/pThreads exploiting CPU Architecture and using CUDA on GPUS. The aim is to compare implementation complexity, speed and resource efficiency.\r\n\r\n###BACKGROUND:\r\nProcessing Real World graphs is an active research problem. The real-world complex graphs are typically very large (with millions or more vertices) and their sizes grow over time. Some researchers predict that the size of these graphs will eventually reach 10^15 vertices. Unfortunately, we do not have publicly available real graphs that are large enough to test the functionality and true scalability of the graph applications. Hence, graph generation is an important field of research. There are many implementation and algorithms that exist but a serious drawback of these models is that they are all sequential models and thus, are inadequate in their usage to generate massive graphs with billions of vertices and edges.\r\n- Stochastic Kronecker Graph (A generalization of RMAT graph generation and ER)\r\n- Erdos-Renyi graph\r\n- Parallel Barabasi-Albert (PBA) method\r\n- CL method (Chung Lu)\r\n\r\nTo concentrate best on our aim of comparison of the implementation libraries, we implemented the SKG algorithm which is a generalization on the ER algorithm. The idea was to best optimize this algorithm over the various libraries to analyze the implementation and speedup.\r\n\r\nWe used SNAP and PaRMAT libraries for initial benchmarking and as code base. The evaluation of our algorithms and benchmarking was implemented by us. The checker functions to ensure that the properties for RMATs were ported from SNAP. Both these libraries provide different options for generations of graphs (directed vs. undirected, sorted vs. unsorted), we modeled our interface to support the same. To get optimal performance, we tuned our kernels/functions to take such parameters in to account.\r\n\r\n###THE CHALLENGE AND THE TACKLE\r\nOur project is different from most other tasks handled in this course. There is no reading of input data, rather a large amount of data is generated. Since each node is operated on independently, there is a large communication overhead to ensure connectivity amongst the nodes. Also, there is an inherent workload imbalance, the implementation had to be tuned to that particular use-cases. Along with that we ensured the following:\r\n\r\n1. Reducing communication between different parallel elements\r\n2. Distributing graph over the cores (load balancing)\r\n3. Writing efficiently to memory\r\n4. Compressed representation of the graph\r\n\r\n##APPROACH\r\nThere are three parts in the approach we adopted:\r\n(1) A Serial CPU Square Generation:\r\nAll three implementations share a serial section in which the input matrix of edges to be generated is divided up in squares according to the probability distributions and these squares get a connection degree associated with them. The division can be seen in the figure.\r\n\r\n![SQ](https://github.com/AthelasCo/Athelas/blob/gh-pages/images/SQ.png?raw=true)\r\n\r\nNOTE: In our analysis, we have excluded timing of this section as a fixed overhead.\r\n\r\nThe next part was implemented differently according to the various optimizations available in the three libraries.\r\n\r\n(2.a) PTHREADs based Graph Generation:\r\nWe followed a worker-queue based implementation for both scheduling the generation of the graph and writing it to file. The optimization over the baseline was better load balancing by scheduling squares better and determining size of squares better. The latency of writing to files was hidden by following a producer-consumer queue and the master thread handling all writes.\r\n\r\n(2.b) openMP Graph Generation:\r\nThe openMP implementation was over all the squares in the list, and having a lock over over writing to file. The motivation behind this was to avoid having one thread idle during generation and have threads scheduled out while waiting over the lock so that generation time could be reduced.\r\n\r\n(2.c) CUDA Graph Generation:\r\nThe kernel design for CUDA involved optimizing over sorted vs. non-sorted graph. At the block level we parallelized over all squares and within a block, the parallellization was over all source nodes. The graph was compressed by writing matrix offsets of edges generated to memory. Hence the compression of the graph helped in reducing the memory transfer overhead from the device. At the same time, the cost of expanding the graph is not much in the CPU. The serial algorithm for sorting was optimized to gain block locality and benefit from shared memory accesses as opposed to flushing to global memory. Another overhead in CUDA was ensuring the random number generation for which we had to initialize different states for graph depth and thread ID. The CURAND library was used for generating a random number from within the kernel.\r\n\r\n###RESULTS\r\n![Bug in code](https://github.com/AthelasCo/Athelas/blob/gh-pages/images/fused.png?raw=true)\r\nWe can see that the original implementation(in red) had a bug and did not actually get the in-degree  curve that we were looking for in a RMAT graph(our implementation in purple). This was an algorithmic change that was needed to ensure correctness as reported by SNAP. The same is ensured across implementations.\r\n\r\nNOTE: Aside on Timing information:\r\n\r\n1. OpenMP and PTHREADs have only the graph generation time mentioned with FILE IO being kept separate. We are in the process of optimizing FILE IO for these and the times will be updated over the weekend.\r\n2. CUDA runtimes includes time for generation and the data transfer time HtoD. As above FILE IO is in the process of being optimized through parallel stream execution and will be updated over the weekend.\r\n3. All profiling done w=using CycleTimer in CPU and NVPROF on GPU. Graph being generated is with 100k vertices and 1 million edges unless stated otherwise.\r\n\r\n\r\n![WL](https://github.com/AthelasCo/Athelas/blob/gh-pages/images/compare.png?raw=true)\r\nThe workload distribution across probability distribution can be seen above. As mentioned before, the target probability matrix plays a significant role in the distribution of work amongst the squares generated and this is a trend across libraries.\r\n\r\nWe are not comparing generation speedup w.r.t. the baseline input as there were some logical faults in the graph generation algorithm, however, we can compare our algorithm across the 3 libraries as its uniform and correct. As can be seen PTHREADs is having an advantage over openMP due to the better distribution and no overheads. CUDA, even with the device transfer is performing an order faster than both.\r\n\r\n![CUDATime](https://github.com/AthelasCo/Athelas/blob/gh-pages/images/runtime.png?raw=true)\r\nThe distribution of the Kernel runtime to device transfer times indicates that as the workload becomes more uneven the kernel takes more time to execute, similarly, as the graph becomes bigger, the data times start dominating. The runtimes are linearly increasing as the no. of nodes increase which is also expected as the each kernel instance will execute for longer uniformly\r\n![CUDAScale](https://github.com/AthelasCo/Athelas/blob/gh-pages/images/scaling2.png?raw=true)\r\n![CUDAScale](https://github.com/AthelasCo/Athelas/blob/gh-pages/images/scaling.png?raw=true)\r\n\r\nAside on programming complexity:\r\n\r\n1. Start-to-Basic Implementation\r\nOpenMP was the easiest to parallelize a given serial implementation, with PTHREADs involving some restructuring to enable threading and queueing. CUDA was complex to get going due to kernel design overheads, figuring the data transfer and parameter(s) access across devices, as well as random number generation on the GPU.\r\n\r\n2. Optimization Freedom\r\nDue to its nature, openMP provided the least options to explore and least flexibility in terms of implementation. CUDA on the other hand is something we are still optimizing over. The chance to exploit block level locality presents many diverse implementation strategies and options.\r\n\r\n###FUTURE WORK\r\nWe want to explore latency hiding in all three for faster FILE IO. The use of compression was already implemented in CUDA and needs to be done in the other two. CUDA also has parallel stream execution by which we can transfer chunks of memory the moment a kernel gets over. We will try to finish the experiments by 5/9.\r\n\r\n###Tl;Dr\r\nWe implemented a Parallel Graph Generation Library which enables a user to generate massive random graphs using the platform of choice. The implementation across libraries was to best understand the complexity as well as optimization capabilities of each and not just to present as an option to the end user. The library scales across graph sizes and is easy and intuitive to setup and use.\r\n\r\n###RESOURCES\r\n- We will be using the code present here as our starting point:\r\n[\"GitHub - ParMAT\"](https://github.com/farkhor/PaRMAT)\r\n- The details of which are present in this paper:\r\n[Chakrabarti, Deepayan, Yiping Zhan, and Christos Faloutsos. \"R-MAT: A Recursive Model for Graph Mining.\" SDM. Vol. 4. 2004.](http://www.cs.ucr.edu/~gupta/research/Publications/Comp/wsvr.pdf)\r\nKhorasani, Farzad, Rajiv Gupta, and Laxmi N. Bhuyan. \"Scalable SIMD-Efficient Graph Processing on GPUs. \r\nand\r\n- SNAP, a library out of STANFORD which is based on OpenMPI and we will be referencing their solution as well:\r\n- [\"GitHub - snap-stanford/snap: Stanford Network Analysis ...\" 2012. 2 Apr. 2016](https://github.com/snap-stanford/snap/blob/master/examples/graphgen/graphgen.cpp)\r\n- [SNAP: Stanford Network Analysis Project.\" 2009. 2 Apr. 2016](http://snap.stanford.edu/class/cs224w-2012/projects/cs224w-035-final.v01.pdf)\r\n\r\n- Our initial understanding of RMATs was influenced by \r\n[Chakrabarti, Deepayan, Yiping Zhan, and Christos Faloutsos. \"R-MAT: A Recursive Model for Graph Mining.\" SDM. Vol. 4. 2004.](http://www.cs.cmu.edu/~christos/PUBLICATIONS/siam04.pdf)\r\n\r\n- And we will be referencing the following two papers for implementing a parallel Kronecker Graph:\r\n-[ Yoo, Andy, and Keith Henderson. \"Parallel generation of massive scale-free graphs.\" arXiv preprint arXiv:1003.3684 (2010)] (http://arxiv.org/pdf/1003.3684v1.pdf%20Parallel%20Generation%20of%20Massive%20Scale-Free%20Graphs)\r\n- [Nobari, Sadegh, et al. \"Fast random graph generation.\" Proceedings of the 14th international conference on extending database technology. ACM, 2011] (http://karras.rutgers.edu/rg.pdf)\r\n\r\n###Work Division\r\nEqual work was performed by both project members.\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}