{
  "name": "Athelas",
  "tagline": "",
  "body": "#A parallel random graph generation library.\r\n                                            By\r\n                            Karandeep Johar & Eshan Verma  \r\n                      kjohar@andrew.cmu.edu   everma@andrew.cmu.edu\r\n\r\n###SUMMARY:\r\nWe will implement a library to generate parallel large-scale graphs using OpenMP exploiting SIMD Architecture and using CUDA on GPUS. The aim is to compare implementation complexity, speed and resource efficiency.\r\n\r\nNOTE: MID_SECTION_REPORT data included [below.](#MID_REPORT_SECTION)\r\n\r\n###BACKGROUND:\r\nProcessing Real World graphs is an active research problem. The real-world complex graphs are typically very large (with millions or more vertices) and their sizes grow over time. Some researchers predict that the size of these graphs will eventually reach 10^15 vertices. Unfortunately, we do not have publicly available real graphs that are large enough to test the functionality and true scalability of the graph applications. Hence, graph generation is an important field of research. There are many implementation and algorithms that exist but a serious drawback of these models is that they are all sequential models and thus, are inadequate in their usage to generate massive graphs with billions of vertices and edges.\r\n- Stochastic Kronecker Graph (A generalization of RMAT graph generation and ER)\r\n- Erdos-Renyi graph\r\n- Parallel Barabasi-Albert (PBA) method\r\n- CL method (Chung Lu)\r\n\r\nThough SKG method generalizes ER there are specific method for ER that can be made to run faster.  We plan to analyze these methods. Each of these methods will present different challenges and we hope to touch upon different aspects of parallel operations.\r\n\r\nTo concentrate best on our aim of comparison of the two implementation libraries, we plan to implement various algorithms of (1) and (2) and optimize the best possible on these algorithms. The covered algorithms will be ER, ZER and PreZER for (2) and their corresponding parallel versions and the serial and parallel implementations of (1). Hence in all we will be implementing 8 algorithms and doing analysis on them.\r\nWe will be using SNAP and PaRMAT libraries for initial benchmarking. The evaluation of our algorithms and benchmarking will be implemented by us. The checker functions will ensure that the properties for RMATs are maintained.\r\nBoth these libraries provide different options for generations of graphs (directed vs. undirected, sorted vs. unsorted), we will be modelling our interface to support the same. To get optimal performance, we plan to tune our kernels/functions to take such parameters in to account.\r\n\r\n###THE CHALLENGE\r\nOur project is different from most other tasks handled in this project. There is no reading of input data rather a large amount of data will be generated. Since each node is operated on independently, there is a large communication overhead to ensure connectivity amongst the nodes. Also, some of the algorithms stated above, can lead to workload imbalance, the implementation will have to be tuned to that particular algorithm. Along with that we will ensure the following:\r\n1. Reducing communication between different parallel \r\n2. Distributing graph over the cores (load balancing)\r\n3. Writing efficiently to memory\r\n4. Exploiting SIMD architecture\r\n\r\nSince this is a project involving analysis of various aspects of CUDA and OpenMP, it will be important to explore all facets of these languages. We aim to have code that reflects the best way to implement a particular algorithm in that library.\r\n\r\n###RESOURCES\r\n- We will be using the code present here as our starting point:\r\n[\"GitHub - snap-stanford/snap: Stanford Network Analysis ...\" 2012. 2 Apr. 2016](https://github.com/farkhor/PaRMAT)\r\n- The details of which are present in this paper:\r\n[Chakrabarti, Deepayan, Yiping Zhan, and Christos Faloutsos. \"R-MAT: A Recursive Model for Graph Mining.\" SDM. Vol. 4. 2004.](http://www.cs.ucr.edu/~gupta/research/Publications/Comp/wsvr.pdf)\r\nKhorasani, Farzad, Rajiv Gupta, and Laxmi N. Bhuyan. \"Scalable SIMD-Efficient Graph Processing on GPUs. \r\nand\r\n- SNAP, a library out of STANFORD which is based on OpenMPI and we will be referencing their solution as well:\r\n- [\"GitHub - snap-stanford/snap: Stanford Network Analysis ...\" 2012. 2 Apr. 2016](https://github.com/snap-stanford/snap/blob/master/examples/graphgen/graphgen.cpp)\r\n- [SNAP: Stanford Network Analysis Project.\" 2009. 2 Apr. 2016](http://snap.stanford.edu/class/cs224w-2012/projects/cs224w-035-final.v01.pdf)\r\n\r\n- Our initial understanding of RMATs was influenced by \r\n[Chakrabarti, Deepayan, Yiping Zhan, and Christos Faloutsos. \"R-MAT: A Recursive Model for Graph Mining.\" SDM. Vol. 4. 2004.](http://www.cs.cmu.edu/~christos/PUBLICATIONS/siam04.pdf)\r\n\r\n- And we will be referencing the following two papers for implementing a parallel Barabasi-Albert method and the Kronecker Graphs:\r\n-[ Yoo, Andy, and Keith Henderson. \"Parallel generation of massive scale-free graphs.\" arXiv preprint arXiv:1003.3684 (2010)] (http://arxiv.org/pdf/1003.3684v1.pdf%20Parallel%20Generation%20of%20Massive%20Scale-Free%20Graphs)\r\n- [Nobari, Sadegh, et al. \"Fast random graph generation.\" Proceedings of the 14th international conference on extending database technology. ACM, 2011] (http://karras.rutgers.edu/rg.pdf)\r\n\r\n- The CL Algorithm is from this paper:\r\n- [Alam, Maksudul, and Maleq Khan. \"Parallel Algorithms for Generating Random Networks with Given Degree Sequences.\" International Journal of Parallel Programming (2014): 1-19.](http://arxiv.org/pdf/1406.1215.pdf)\r\n\r\n###GOALS AND DELIVERABLES. \r\nPLAN TO ACHIEVE: We hope to implement various graph generation models on both OpenMP and CUDA and be more efficient in both of them than compared to the pthread-based starter code.  Our aim is to not blindly focus on the speed of execution but also evaluate various aspects of the problem and the problems faced while parallelizing it over different architectures.\r\nHence we will aim to deep-dive into various aspects of designing over the GPU & OpenMP and using the built-in constructs & pre-existing libraries to best accelerate and optimize the code.\r\nFor the chosen task, the reason for implementing different algorithms is to focus on extracting metrics for comparison and effectively evaluate the two platforms.\r\n\r\n###HOPE TO ACHIEVE: \r\n\r\n1) Make the starter code (pthread based) reflect the algorithms we have implemented and then have an efficient way to compare 3 architectures over different implementations. \r\n\r\n2) Implement BA and CL and add them to the analysis\r\n\r\n###PLATFORM CHOICE\r\nOur graph generation algorithms will be SPMD in nature, which will generate a lot of output data and have high inter-compute core communication. We will use a high count CPU core and A comparable GPU to be able to have an effective comparison. We will develop on GHC machines and plan to test and final benchmarking on latedays.\r\nFine grain optimizations, based on CPU cache sizes or GPU shared memories will be dictated for final testing.\r\n\r\n##MID_REPORT_SECTION\r\n###The Story So Far\r\nStarting with the implementation of PaRMat, we first ensured the correctness of the graph by analyzing the graphs generated by the library using implementation of SNAP and the graph analysis tools present in that. We fixed a sampling bug and added random noise during the sampling phase to better match the degree distribution of the generated graphs to graphs mentioned in the RMAT paper and those generated by SNAP.\r\nFurther, to ensure proper load balancing, especially for skewed probability distribution, we implemented a different thread scheduling scheme and a different rectangle generation scheme to distribute work equally among all threads. To improve the graph generation quality, we also implemented the poisson SKG algorithm.\r\nOnce pThreads implementation was stable and scaling to large graphs and multi-threads. We implemented the OpenMP implementation of the code to evaluate the speedup using the library. The major bottleneck in this approach seems to be FileIO and the time spent by threads in trying to acquire locks to serially write to the file from each thread. As we were optimizing for throughput and memory efficiency, this approach was suited. However, going ahead, we might change this and take a memory hit.\r\n\r\n###Of Goals and Deliveries\r\nAs per the initial published plan, we should have finished an OpenMP implementation of the library. We are a little behind schedule in this regard as we are facing issues with serial fileIO. However, once that is ironed out, we should be able to deliver the initial promised library implementations in openMP and CUDA.\r\nWe want to concentrate on improving the analysis and performance of the current algorithm and not focus on implementing new algorithms. As the scope of the project is to focus on the parallelism aspect of a particular algorithm and not algorithmic improvements themselves, we want to focus on the baseline code that we have and implement as many parallelism smarts.\r\nIn this regard we are still hoping to achieve implementations in openMP and CUDA and have a strong comparison between the two.\r\n\r\n###Show and Tell\r\nFor the parallelism competition, we will be displaying our findings and analysis. We have some plans of building a demo of how each thread is load balancing and building the graph, but that is strictly optional if time permits.\r\n\r\n###Results\r\n![Bug in code](https://github.com/AthelasCo/Athelas/blob/gh-pages/images/fused.png?raw=true)\r\n![Timing results](https://github.com/AthelasCo/Athelas/blob/gh-pages/images/timing.png?raw=true)\r\nWe can see that the original implementation(in red) had a bug and did not actually get the in-degree  curve that we were looking for in a RMAT graph(our implementation in purple). \r\n\r\nThe clock time decreases as we increase the threads to a point after which it increases which may be because of increasing locks contention and insufficient problem size for the number of threads.\r\nWe have a lot of parameters that we can optimize over. Say the user specifies that he wants a sorted output. In that case PKSG algorithm will be a better bet over the other normal algorithm because it outputs the edges sorted by the source vertex. Given fewer number of edges to generate  this method may be slower.  We could also for example instead of generating edges take out edges from the fully connected graph. This would significantly help with reducing collision checking time.\r\n\r\n###Concerns\r\n1. FileIO: By our current estimates, FileIO is about 33\\% of CPU compute time. This can prove to be the long pole as this will prove to be bandwidth bound and can lead to openMP ineffectiveness.\r\n2. Arithmetic Intensity: The arithmetic intensity of the graph generation is rather low. The edge assignment is based on lookups from a probability distribution and recursive assignment to a section of the adjacency matrix. We need to optimize these tasks further.\r\n3. Optimizing over Different Parameters: The graph generation can have many broad categories: directed/undirected, sorted/unsorted, duplicate/non-duplicate edges, size of the graph, self edges, etc. Given so many parameters, it is rather difficult to formulate a general strategy to optimize over all these parameters. \r\n\r\n##SCHEDULE: Updated for Project Checkpoint\r\n* 4/23    : Fix OpenMP Mutex Bottleneck for FileIO\r\n* 4/27    : Initial CUDA Implementation\r\n* 4/29    : Finish all CUDA implementation of the algorithm\r\n* 5/4     : Finish compression for IO\r\n* 5/7     : Finish Benchmarking and finer optimizations.\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}