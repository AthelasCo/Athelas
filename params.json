{
  "name": "Athelas",
  "tagline": "",
  "body": "#A parallel random graph generation library.\r\n                                  By\r\n                   Karandeep Johar & Eshan Verma  \r\n             kjohar@andrew.cmu.edu   everma@andrew.cmu.edu\r\n\r\n###SUMMARY:\r\nWe implemented a library to generate parallel large-scale graphs using OpenMP/pThreads, exploiting CPU Architecture and using CUDA on GPUS. The aim was to compare implementation complexity, speed and resource efficiency.\r\n\r\n###BACKGROUND:\r\nProcessing Real World graphs is an active research problem. The real-world complex graphs are typically very large (with millions or more vertices) and their sizes grow over time. Some researchers predict that the size of these graphs will eventually reach 10^15 vertices. Unfortunately, we do not have publicly available real graphs that are large enough to test the functionality and true scalability of the graph applications. Hence, graph generation is an important field of research. There are many implementation and algorithms that exist but a serious drawback of these models is that they are all sequential models and thus, are inadequate in their usage to generate massive graphs with billions of vertices and edges.\r\n- Stochastic Kronecker Graph (A generalization of RMAT graph generation and ER)\r\n- Erdos-Renyi graph\r\n- Parallel Barabasi-Albert (PBA) method\r\n- CL method (Chung Lu)\r\n\r\nTo concentrate best on our aim of comparison of the implementation libraries, we implemented the RMAT algorithm which is a generalization on the ER algorithm. The idea was to best optimize this algorithm over the various libraries to analyze the implementation and speedup.\r\n\r\nWe used SNAP and PaRMAT libraries for initial benchmarking and as code base. The evaluation of our algorithms and benchmarking was implemented by us. The checker functions to ensure that the properties for RMATs were ported from SNAP. Both these libraries provide different options for generations of graphs (directed vs. undirected, sorted vs. unsorted), we modeled our interface to support the same. To get optimal performance, we tuned our kernels/functions to take such parameters in to account.\r\n\r\n###THE CHALLENGE AND THE TACKLE\r\nOur project is different from most other tasks handled in this course. There is no reading of input data, rather a large amount of data is generated. Since each node is operated on independently, there is a large communication overhead to ensure connectivity amongst the nodes. Also, there is an inherent workload imbalance, the implementation had to be tuned to that particular use-cases. Along with that we ensured the following:\r\n\r\n1. Reducing communication between different parallel elements\r\n2. Distributing graph over the cores (load balancing)\r\n3. Writing efficiently to memory\r\n4. Compressed representation of the graph\r\n\r\n##APPROACH\r\nThere are three parts in the approach we adopted:\r\n(1) A Serial CPU Square Generation:\r\nAll three implementations share a serial section in which the input matrix of edges to be generated is divided up in squares according to the probability distributions and these squares get a connection degree associated with them. The division can be seen in the figure.\r\n\r\n![SQ](https://github.com/AthelasCo/Athelas/blob/gh-pages/images/SQ.png?raw=true)\r\n\r\n*Square Generation across which parallelism is maintained*\r\n\r\nNOTE: In our analysis, we have excluded timing of this section as a fixed overhead.\r\n\r\nThe next part was implemented differently according to the various optimizations available in the three libraries.\r\n\r\n(2.a) PTHREADs based Graph Generation:\r\nWe followed a worker-queue based implementation for both scheduling the generation of the graph and writing it to file. The optimization over the baseline was better load balancing by scheduling squares better and determining size of squares better. The latency of writing to files was hidden by following a producer-consumer queue and the master thread handling all writes.\r\n\r\n(2.b) openMP Graph Generation:\r\nThe openMP implementation was over all the squares in the list, and having a lock over over writing to file. The motivation behind this was to avoid having one thread idle during generation and have threads scheduled out while waiting over the lock so that generation time could be reduced.\r\n\r\n(2.c) CUDA Graph Generation:\r\nThe kernel design for CUDA involved optimizing over sorted vs. non-sorted graph. At the block level we parallelized over all squares and within a block, the parallellization was over all source nodes. The graph was compressed by writing matrix offsets of edges generated to memory. Hence the compression of the graph helped in reducing the memory transfer overhead from the device. At the same time, the cost of expanding the graph is not much in the CPU. The serial algorithm for sorting was optimized to gain block locality and benefit from shared memory accesses as opposed to flushing to global memory. Another overhead in CUDA was ensuring the random number generation for which we had to initialize different states for graph depth and thread ID. The CURAND library was used for generating a random number from within the kernel.\r\n\r\n###RESULTS\r\n![Bug in code](https://github.com/AthelasCo/Athelas/blob/gh-pages/images/fused.png?raw=true)\r\n\r\n*Error correction in original code*\r\n\r\nWe can see that the original implementation(in red) had a bug and did not actually get the in-degree  curve that we were looking for in a RMAT graph(our implementation in purple). This was an algorithmic change that was needed to ensure correctness as reported by SNAP. The same is ensured across implementations.\r\n\r\nNOTE: Aside on Timing information:\r\n\r\n1. OpenMP and PTHREADs have only the graph generation time mentioned with FILE IO being kept separate. We are in the process of optimizing FILE IO for these and the times will be updated over the weekend.\r\n2. CUDA runtimes includes time for generation and the data transfer time HtoD. As above FILE IO is in the process of being optimized through parallel stream execution and will be updated over the weekend.\r\n3. All profiling done was using CycleTimer in CPU and NVPROF on GPU. Graph being generated is with 100k vertices and 1 million edges unless stated otherwise.\r\n4. The comparisons are pre-compression. Comparison data is presented for CUDA in the last section.\r\n\r\n![TS](https://github.com/AthelasCo/Athelas/blob/gh-pages/images/threadScaling.png?raw=true)\r\n\r\n*Scaling of code across nCores of CPU*\r\n\r\nOur implementation scales across cores and plateaus after a point as the inherent parallelism breaks down.\r\n\r\n![WL](https://github.com/AthelasCo/Athelas/blob/gh-pages/images/compare.png?raw=true)\r\n\r\n*Workload Distribution of implementation across probabilities*\r\n\r\nThe workload distribution across probability distribution can be seen above. As mentioned before, the target probability matrix plays a significant role in the distribution of work amongst the squares generated and this is a trend across libraries.\r\n\r\nWe are not comparing generation speedup w.r.t. the baseline input as there were some logical faults in the graph generation algorithm, however, we can compare our algorithm across the 3 libraries as its uniform and correct. As can be seen PTHREADs is having an advantage over openMP due to the better distribution and no overheads. CUDA, even with the device transfer is performing an order faster than both.\r\n\r\n![CUDATime](https://github.com/AthelasCo/Athelas/blob/gh-pages/images/runtime.png?raw=true)\r\n\r\n*CUDA runtimes on different probability distribution*\r\n\r\nThe distribution of the Kernel runtime to device transfer times indicates that as the workload becomes more uneven the kernel takes more time to execute, similarly, as the graph becomes bigger, the data times start dominating. The runtimes are linearly increasing as the no. of nodes increase which is also expected as the each kernel instance will execute for longer uniformly\r\n![CUDAScale](https://github.com/AthelasCo/Athelas/blob/gh-pages/images/scaling2.png?raw=true)\r\n![CUDAScale](https://github.com/AthelasCo/Athelas/blob/gh-pages/images/scaling.png?raw=true)\r\n\r\n*CUDA Kernel Vs. Data Transfer Timings across Graph Sizes*\r\n\r\n####Compression\r\n![CUDACompress](https://github.com/AthelasCo/Athelas/blob/gh-pages/images/compress.png?raw=true)\r\nAfter looking at the memory transfer times we realized that memory copy times are taking a significant amount of time on the GPU. We first looked at the sqaures and saw that most ranges were less than 2^16.  This meant that we can actually fit in the the edge exactly in a 2^32 bit representations of uints. \r\nThe idea here was to build a compact representation of the graph so as to minimize the data transfer between device and host. By storing edges in a compact form, we were able to reduce the memory transfer by a factor of 2x on average which is a significant saving. This representation has minimal overhead in the kernel or in terms of reconstruction on the CPU. The basic representation is by storing an edge tuple as a single number instead of a tuple. Within a square each edge can be represented as an offset form the origin, the details of this representation are listed in the Algorithm description below.\r\n![CUDACompress](https://github.com/AthelasCo/Athelas/blob/gh-pages/images/Algo.png?raw=true)\r\n\r\n*Algorithm for Compression*\r\n\r\nThis also allows for easy reconstruction, as the math pre edge is independent and simple. Thus instead of storing an edge as a tuple, we are storing it as a single integer.\r\n\r\n####Huffman coding\r\n\r\nWe looked at the results of this optimization and found that as we increase the number of vertices the number of times that we can actually apply this approach goes down. This is because the range of the square quickly exceeds 2^16 vertices. We then looked at another method for achieving compression.\r\n\r\nUnfortunately most other compression formats are ill suited for compressing the graph on the fly as we are generating it. So formats like CSR which are are commonly used in numpy are ill suited for the job.\r\n\r\nThe key insight is that all edges are not equally likely and hence our level of our surprise should be different for different edges. It should be lower for edges with higher probabilities and higher for ones with lower probabilities. Huffman coding actually formalizes this concept. \r\n\r\nWe can think of an edge (V1, V2) as a series of choices. At each stage we choose one of the 4 blocks. A,B,C or D. So we can actually represent the edge as [A|B|C|D]+ of length log(|V|). Now, because we know the prior probabilities of a,b,c and d we can encode this string using huffman codes for the the respective blocks. \r\n\r\nSay A has a probability mass of 0.9. That would mean that most of the edges would lie in the upper left corner of the graph. This would mean that most edges have a large percentage of As in them. This would give us a huge reason for compression. \r\n\r\n<<<<INSERT GRAPH HERE>>>\r\n\r\nUnsurprisingly the compression ratio for graphs is higher for the cases where the probabilities are really skewed or the number of vertices is smaller than 2^32. The second condition follows from the fact that greater the number of vertices the greater the depth we need to go to actually get the graph. This implies a greater edge string length in terms of blocks for those  graphs and lesser scope for compression.\r\n\r\nAs we increase the number of edges the compression ratio remains constant. This is because the compression ratio per edge is independent of the number of edges.\r\n\r\n\r\nAside on programming complexity:\r\n\r\n1. Start-to-Basic Implementation\r\nOpenMP was the easiest to parallelize a given serial implementation, with PTHREADs involving some restructuring to enable threading and queueing. CUDA was complex to get going due to kernel design overheads, figuring the data transfer and parameter(s) access across devices, as well as random number generation on the GPU.\r\n\r\n2. Optimization Freedom\r\nDue to its nature, openMP provided the least options to explore and least flexibility in terms of implementation. CUDA on the other hand is something we are still optimizing over. The chance to exploit block level locality presents many diverse implementation strategies and options.\r\n\r\n###FUTURE WORK\r\nWe want to explore latency hiding in all three for faster memory operations. The use of compression was already implemented in CUDA and needs to be done in the other two. CUDA also has parallel stream execution by which we can transfer chunks of memory the moment a kernel gets over. We will try to finish the experiments by 5/9.\r\n\r\n###Tl;Dr\r\nWe implemented a Parallel Graph Generation Library which enables a user to generate massive random graphs using the platform of choice. The implementation across libraries was to best understand the complexity as well as optimization capabilities of each and not just to present as an option to the end user. The library scales across graph sizes and is easy and intuitive to setup and use.\r\n\r\n###RESOURCES\r\n- We will be using the code present here as our starting point:\r\n[\"GitHub - ParMAT\"](https://github.com/farkhor/PaRMAT)\r\n- The details of which are present in this paper:\r\n[Chakrabarti, Deepayan, Yiping Zhan, and Christos Faloutsos. \"R-MAT: A Recursive Model for Graph Mining.\" SDM. Vol. 4. 2004.](http://www.cs.ucr.edu/~gupta/research/Publications/Comp/wsvr.pdf)\r\nKhorasani, Farzad, Rajiv Gupta, and Laxmi N. Bhuyan. \"Scalable SIMD-Efficient Graph Processing on GPUs. \r\nand\r\n- SNAP, a library out of STANFORD which is based on OpenMPI and we will be referencing their solution as well:\r\n- [\"GitHub - snap-stanford/snap: Stanford Network Analysis ...\" 2012. 2 Apr. 2016](https://github.com/snap-stanford/snap/blob/master/examples/graphgen/graphgen.cpp)\r\n- [SNAP: Stanford Network Analysis Project.\" 2009. 2 Apr. 2016](http://snap.stanford.edu/class/cs224w-2012/projects/cs224w-035-final.v01.pdf)\r\n\r\n- Our initial understanding of RMATs was influenced by \r\n[Chakrabarti, Deepayan, Yiping Zhan, and Christos Faloutsos. \"R-MAT: A Recursive Model for Graph Mining.\" SDM. Vol. 4. 2004.](http://www.cs.cmu.edu/~christos/PUBLICATIONS/siam04.pdf)\r\n\r\n- And we will be referencing the following two papers for implementing a parallel Kronecker Graph:\r\n-[ Yoo, Andy, and Keith Henderson. \"Parallel generation of massive scale-free graphs.\" arXiv preprint arXiv:1003.3684 (2010)] (http://arxiv.org/pdf/1003.3684v1.pdf%20Parallel%20Generation%20of%20Massive%20Scale-Free%20Graphs)\r\n- [Nobari, Sadegh, et al. \"Fast random graph generation.\" Proceedings of the 14th international conference on extending database technology. ACM, 2011] (http://karras.rutgers.edu/rg.pdf)\r\n\r\n###Work Division\r\nEqual work was performed by both project members.\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}