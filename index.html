<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Athelas by AthelasCo</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Athelas</h1>
        <p></p>

        <p class="view"><a href="https://github.com/AthelasCo/Athelas">View the Project on GitHub <small>AthelasCo/Athelas</small></a></p>


        <ul>
          <li><a href="https://github.com/AthelasCo/Athelas/zipball/master">Download <strong>ZIP File</strong></a></li>
          <li><a href="https://github.com/AthelasCo/Athelas/tarball/master">Download <strong>TAR Ball</strong></a></li>
          <li><a href="https://github.com/AthelasCo/Athelas">View On <strong>GitHub</strong></a></li>
        </ul>
      </header>
      <section>
        <h1>
<a id="a-parallel-random-graph-generation-library" class="anchor" href="#a-parallel-random-graph-generation-library" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>A parallel random graph generation library.</h1>

<pre><code>                              By
               Karandeep Johar &amp; Eshan Verma  
         kjohar@andrew.cmu.edu   everma@andrew.cmu.edu
</code></pre>

<h3>
<a id="summary" class="anchor" href="#summary" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>SUMMARY:</h3>

<p>We will implement a library to generate parallel large-scale graphs using OpenMP/pThreads exploiting CPU Architecture and using CUDA on GPUS. The aim is to compare implementation complexity, speed and resource efficiency.</p>

<h3>
<a id="background" class="anchor" href="#background" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>BACKGROUND:</h3>

<p>Processing Real World graphs is an active research problem. The real-world complex graphs are typically very large (with millions or more vertices) and their sizes grow over time. Some researchers predict that the size of these graphs will eventually reach 10^15 vertices. Unfortunately, we do not have publicly available real graphs that are large enough to test the functionality and true scalability of the graph applications. Hence, graph generation is an important field of research. There are many implementation and algorithms that exist but a serious drawback of these models is that they are all sequential models and thus, are inadequate in their usage to generate massive graphs with billions of vertices and edges.</p>

<ul>
<li>Stochastic Kronecker Graph (A generalization of RMAT graph generation and ER)</li>
<li>Erdos-Renyi graph</li>
<li>Parallel Barabasi-Albert (PBA) method</li>
<li>CL method (Chung Lu)</li>
</ul>

<p>To concentrate best on our aim of comparison of the implementation libraries, we implemented the SKG algorithm which is a generalization on the ER algorithm. The idea was to best optimize this algorithm over the various libraries to analyze the implementation and speedup.</p>

<p>We used SNAP and PaRMAT libraries for initial benchmarking and as code base. The evaluation of our algorithms and benchmarking was implemented by us. The checker functions to ensure that the properties for RMATs were ported from SNAP. Both these libraries provide different options for generations of graphs (directed vs. undirected, sorted vs. unsorted), we modeled our interface to support the same. To get optimal performance, we tuned our kernels/functions to take such parameters in to account.</p>

<h3>
<a id="the-challenge-and-the-tackle" class="anchor" href="#the-challenge-and-the-tackle" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>THE CHALLENGE AND THE TACKLE</h3>

<p>Our project is different from most other tasks handled in this course. There is no reading of input data, rather a large amount of data is generated. Since each node is operated on independently, there is a large communication overhead to ensure connectivity amongst the nodes. Also, there is an inherent workload imbalance, the implementation had to be tuned to that particular use-cases. Along with that we ensured the following:</p>

<ol>
<li>Reducing communication between different parallel elements</li>
<li>Distributing graph over the cores (load balancing)</li>
<li>Writing efficiently to memory</li>
<li>Compressed representation of the graph</li>
</ol>

<h2>
<a id="approach" class="anchor" href="#approach" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>APPROACH</h2>

<p>There are three parts in the approach we adopted:
(1) A Serial CPU Square Generation:
All three implementations share a serial section in which the input matrix of edges to be generated is divided up in squares according to the probability distributions and these squares get a connection degree associated with them. The division can be seen in the figure.</p>

<p><img src="https://github.com/AthelasCo/Athelas/blob/gh-pages/images/SQ.png?raw=true" alt="SQ"></p>

<p><em>Square Generation across which parallelism is maintained</em></p>

<p>NOTE: In our analysis, we have excluded timing of this section as a fixed overhead.</p>

<p>The next part was implemented differently according to the various optimizations available in the three libraries.</p>

<p>(2.a) PTHREADs based Graph Generation:
We followed a worker-queue based implementation for both scheduling the generation of the graph and writing it to file. The optimization over the baseline was better load balancing by scheduling squares better and determining size of squares better. The latency of writing to files was hidden by following a producer-consumer queue and the master thread handling all writes.</p>

<p>(2.b) openMP Graph Generation:
The openMP implementation was over all the squares in the list, and having a lock over over writing to file. The motivation behind this was to avoid having one thread idle during generation and have threads scheduled out while waiting over the lock so that generation time could be reduced.</p>

<p>(2.c) CUDA Graph Generation:
The kernel design for CUDA involved optimizing over sorted vs. non-sorted graph. At the block level we parallelized over all squares and within a block, the parallellization was over all source nodes. The graph was compressed by writing matrix offsets of edges generated to memory. Hence the compression of the graph helped in reducing the memory transfer overhead from the device. At the same time, the cost of expanding the graph is not much in the CPU. The serial algorithm for sorting was optimized to gain block locality and benefit from shared memory accesses as opposed to flushing to global memory. Another overhead in CUDA was ensuring the random number generation for which we had to initialize different states for graph depth and thread ID. The CURAND library was used for generating a random number from within the kernel.</p>

<h3>
<a id="results" class="anchor" href="#results" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>RESULTS</h3>

<p><img src="https://github.com/AthelasCo/Athelas/blob/gh-pages/images/fused.png?raw=true" alt="Bug in code"></p>

<p><em>Error correction in original code</em></p>

<p>We can see that the original implementation(in red) had a bug and did not actually get the in-degree  curve that we were looking for in a RMAT graph(our implementation in purple). This was an algorithmic change that was needed to ensure correctness as reported by SNAP. The same is ensured across implementations.</p>

<p>NOTE: Aside on Timing information:</p>

<ol>
<li>OpenMP and PTHREADs have only the graph generation time mentioned with FILE IO being kept separate. We are in the process of optimizing FILE IO for these and the times will be updated over the weekend.</li>
<li>CUDA runtimes includes time for generation and the data transfer time HtoD. As above FILE IO is in the process of being optimized through parallel stream execution and will be updated over the weekend.</li>
<li>All profiling done was using CycleTimer in CPU and NVPROF on GPU. Graph being generated is with 100k vertices and 1 million edges unless stated otherwise.</li>
<li>The comparisons are pre-compression. Comparison data is presented for CUDA in the last section.</li>
</ol>

<p><img src="https://github.com/AthelasCo/Athelas/blob/gh-pages/images/threadScaling.png?raw=true" alt="TS"></p>

<p><em>Scaling of code across nCores of CPU</em></p>

<p>Our implementation scales across cores and plateaus after a point as the inherent parallelism breaks down.</p>

<p><img src="https://github.com/AthelasCo/Athelas/blob/gh-pages/images/compare.png?raw=true" alt="WL"></p>

<p><em>Workload Distribution of implementation across probabilities</em></p>

<p>The workload distribution across probability distribution can be seen above. As mentioned before, the target probability matrix plays a significant role in the distribution of work amongst the squares generated and this is a trend across libraries.</p>

<p>We are not comparing generation speedup w.r.t. the baseline input as there were some logical faults in the graph generation algorithm, however, we can compare our algorithm across the 3 libraries as its uniform and correct. As can be seen PTHREADs is having an advantage over openMP due to the better distribution and no overheads. CUDA, even with the device transfer is performing an order faster than both.</p>

<p><img src="https://github.com/AthelasCo/Athelas/blob/gh-pages/images/runtime.png?raw=true" alt="CUDATime"></p>

<p><em>CUDA runtimes on different probability distribution</em></p>

<p>The distribution of the Kernel runtime to device transfer times indicates that as the workload becomes more uneven the kernel takes more time to execute, similarly, as the graph becomes bigger, the data times start dominating. The runtimes are linearly increasing as the no. of nodes increase which is also expected as the each kernel instance will execute for longer uniformly
<img src="https://github.com/AthelasCo/Athelas/blob/gh-pages/images/scaling2.png?raw=true" alt="CUDAScale">
<img src="https://github.com/AthelasCo/Athelas/blob/gh-pages/images/scaling.png?raw=true" alt="CUDAScale"></p>

<p><em>CUDA Kernel Vs. Data Transfer Timings across Graph Sizes</em></p>

<h4>
<a id="compression" class="anchor" href="#compression" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Compression</h4>

<p><img src="https://github.com/AthelasCo/Athelas/blob/gh-pages/images/compress.png?raw=true" alt="CUDACompress">
The idea here was to build a compact representation of the graph so as to minimize the data transfer between device and host. By storing edges in a compact form, we were able to reduce the memory transfer by a factor of 3x on average which is a significant saving. This representation has minimal overhead in the kernel or in terms of reconstruction on the CPU. The basic representation is by storing an edge tuple as a single number instead of a tuple. Within a square each edge can be represented as an offset form the origin, the details of this representation are listed in the Algorithm description below.
<img src="https://github.com/AthelasCo/Athelas/blob/gh-pages/images/Algo.png?raw=true" alt="CUDACompress"></p>

<p><em>Algorithm for Compression</em></p>

<p>This also allows for easy reconstruction, as the math pre edge is independent and simple. Thus instead of storing an edge as a tuple, we are storing it as a single integer.</p>

<p>Aside on programming complexity:</p>

<ol>
<li><p>Start-to-Basic Implementation
OpenMP was the easiest to parallelize a given serial implementation, with PTHREADs involving some restructuring to enable threading and queueing. CUDA was complex to get going due to kernel design overheads, figuring the data transfer and parameter(s) access across devices, as well as random number generation on the GPU.</p></li>
<li><p>Optimization Freedom
Due to its nature, openMP provided the least options to explore and least flexibility in terms of implementation. CUDA on the other hand is something we are still optimizing over. The chance to exploit block level locality presents many diverse implementation strategies and options.</p></li>
</ol>

<h3>
<a id="future-work" class="anchor" href="#future-work" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>FUTURE WORK</h3>

<p>We want to explore latency hiding in all three for faster FILE IO. The use of compression was already implemented in CUDA and needs to be done in the other two. CUDA also has parallel stream execution by which we can transfer chunks of memory the moment a kernel gets over. We will try to finish the experiments by 5/9.</p>

<h3>
<a id="tldr" class="anchor" href="#tldr" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Tl;Dr</h3>

<p>We implemented a Parallel Graph Generation Library which enables a user to generate massive random graphs using the platform of choice. The implementation across libraries was to best understand the complexity as well as optimization capabilities of each and not just to present as an option to the end user. The library scales across graph sizes and is easy and intuitive to setup and use.</p>

<h3>
<a id="resources" class="anchor" href="#resources" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>RESOURCES</h3>

<ul>
<li>We will be using the code present here as our starting point:
<a href="https://github.com/farkhor/PaRMAT">"GitHub - ParMAT"</a>
</li>
<li>The details of which are present in this paper:
<a href="http://www.cs.ucr.edu/%7Egupta/research/Publications/Comp/wsvr.pdf">Chakrabarti, Deepayan, Yiping Zhan, and Christos Faloutsos. "R-MAT: A Recursive Model for Graph Mining." SDM. Vol. 4. 2004.</a>
Khorasani, Farzad, Rajiv Gupta, and Laxmi N. Bhuyan. "Scalable SIMD-Efficient Graph Processing on GPUs. 
and</li>
<li>SNAP, a library out of STANFORD which is based on OpenMPI and we will be referencing their solution as well:</li>
<li><a href="https://github.com/snap-stanford/snap/blob/master/examples/graphgen/graphgen.cpp">"GitHub - snap-stanford/snap: Stanford Network Analysis ..." 2012. 2 Apr. 2016</a></li>
<li><p><a href="http://snap.stanford.edu/class/cs224w-2012/projects/cs224w-035-final.v01.pdf">SNAP: Stanford Network Analysis Project." 2009. 2 Apr. 2016</a></p></li>
<li><p>Our initial understanding of RMATs was influenced by 
<a href="http://www.cs.cmu.edu/%7Echristos/PUBLICATIONS/siam04.pdf">Chakrabarti, Deepayan, Yiping Zhan, and Christos Faloutsos. "R-MAT: A Recursive Model for Graph Mining." SDM. Vol. 4. 2004.</a></p></li>
<li><p>And we will be referencing the following two papers for implementing a parallel Kronecker Graph:
-<a href="http://arxiv.org/pdf/1003.3684v1.pdf%20Parallel%20Generation%20of%20Massive%20Scale-Free%20Graphs"> Yoo, Andy, and Keith Henderson. "Parallel generation of massive scale-free graphs." arXiv preprint arXiv:1003.3684 (2010)</a></p></li>
<li><a href="http://karras.rutgers.edu/rg.pdf">Nobari, Sadegh, et al. "Fast random graph generation." Proceedings of the 14th international conference on extending database technology. ACM, 2011</a></li>
</ul>

<h3>
<a id="work-division" class="anchor" href="#work-division" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Work Division</h3>

<p>Equal work was performed by both project members.</p>
      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/AthelasCo">AthelasCo</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    
  </body>
</html>
